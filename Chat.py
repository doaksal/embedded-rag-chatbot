# chat.py
# Akbank Generative AI Bootcamp â€“ Embedded RAG + Gemini 2.0 Flash
# GeliÅŸtiren: Doga
# AÃ§Ä±klama:
# Bu proje, lokal tÄ±bbi makaleleri kullanarak embedding tabanlÄ± retrieval (RAG) yapar
# ve Gemini 2.0 Flash modelini kullanarak akÄ±llÄ± yanÄ±tlar Ã¼retir.
# Veri kaynaÄŸÄ± lokal olduÄŸu iÃ§in internet baÄŸlantÄ±sÄ± gerekmez.
# ChromaDB ile vektÃ¶r veritabanÄ± kurulmuÅŸ ve embeddingâ€™ler saklanmÄ±ÅŸtÄ±r.

import os
from dotenv import load_dotenv
import google.generativeai as genai
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import chromadb

# ---------------------------
# 1ï¸âƒ£ API Key ve model kurulumu
# ---------------------------
# .env dosyasÄ±ndaki API_KEY kullanÄ±lÄ±r. Yoksa hata verir.
load_dotenv()
API = os.getenv("API_KEY")
if not API:
    raise ValueError("API_KEY bulunamadÄ±! .env dosyanÄ± ve konumunu kontrol et.")

# Gemini 2.0 Flash yapÄ±landÄ±rmasÄ±
genai.configure(api_key=API)
model = genai.GenerativeModel("gemini-2.0-flash")
chat = model.start_chat(history=[])

# ---------------------------
# 2ï¸âƒ£ Lokal dataset yÃ¼kleme
# ---------------------------
# Data klasÃ¶rÃ¼ndeki tÃ¼m parquet dosyalarÄ± yÃ¼klenir
print("ğŸ“¥ Dataset yÃ¼kleniyor (lokal dosyalardan)...")
files = {
    "acibadem": "data/acibadem-00000-of-00001.parquet",
    "anadolusaglik": "data/anadolusaglik-00000-of-00001.parquet",
    "atlas": "data/atlas-00000-of-00001.parquet",
    "baskentistanbul": "data/baskentistanbul-00000-of-00001.parquet",
    "bayindir": "data/bayindir-00000-of-00001.parquet",
    "florence": "data/florence-00000-of-00001.parquet",
    "guven": "data/guven-00000-of-00001.parquet",
    "liv": "data/liv-00000-of-00001.parquet",
    "medicalpark": "data/medicalpark-00000-of-00001.parquet",
    "medicalpoint": "data/medicalpoint-00000-of-00001.parquet",
    "medicana": "data/medicana-00000-of-00001.parquet",
    "medipol": "data/medipol-00000-of-00001.parquet",
    "memorial": "data/memorial-00000-of-00001.parquet",
    "yeditepe": "data/yeditepe-00000-of-00001.parquet"
}

# Datasets kÃ¼tÃ¼phanesi ile parquet dosyalarÄ± okunur
dataset_raw = load_dataset("parquet", data_files=files)

# Sadece text alanlarÄ± ve baÅŸlÄ±klarÄ± ayÄ±klÄ±yoruz (test iÃ§in ilk 3 kayÄ±t alÄ±nÄ±yor)
dataset = []
titles = []
for split in dataset_raw:
    dataset += dataset_raw[split][:3]["text"]
    titles += ["Makale " + str(i+1) for i in range(len(dataset_raw[split][:3]["text"]))]

print(f"âœ… Dataset yÃ¼klendi. Toplam {len(dataset)} makale var.")

# ---------------------------
# 3ï¸âƒ£ Embedding ve ChromaDB
# ---------------------------
# SentenceTransformer ile embedding oluÅŸturulur
print("ğŸ“¦ Embedding modeli yÃ¼kleniyor...")
embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# ChromaDB client oluÅŸturulur ve koleksiyon kurulur
print("ğŸ’¾ ChromaDB kuruluyor ve koleksiyon oluÅŸturuluyor...")
client = chromadb.Client()
collection = client.get_or_create_collection("medical_articles")

# EÄŸer koleksiyon boÅŸsa embeddingâ€™leri hesapla ve ekle
if len(collection.get()["ids"]) == 0:
    print("ğŸ”— Makaleler embeddingâ€™e Ã§evriliyor ve veritabanÄ±na ekleniyor...")
    embeddings = embedding_model.encode(dataset, batch_size=32, show_progress_bar=True)

    for i, (text, title, emb) in enumerate(zip(dataset, titles, embeddings)):
        collection.add(
            ids=[str(i)],
            embeddings=[emb],
            documents=[text],
            metadatas=[{"title": title}]
        )
    print("âœ… TÃ¼m makaleler veritabanÄ±na eklendi.")
else:
    print("âœ… Koleksiyon zaten dolu, embeddingâ€™ler tekrar hesaplanmadÄ±.")

# ---------------------------
# 4ï¸âƒ£ GeliÅŸmiÅŸ retrieval fonksiyonu
# ---------------------------
def retrieve_context(query, top_k=42):
    """
    - Embedding tabanlÄ± en yakÄ±n makaleleri getirir.
    - EÄŸer query belirli bir konu iÃ§eriyorsa (Ã¶r. 'grip'), title bazlÄ± filtre uygular.
    - DÃ¶ndÃ¼rÃ¼len metinler Ã¶zetlenir (ilk 300 karakter) promptâ€™u ÅŸiÅŸirmemek iÃ§in.
    """
    all_docs = collection.get()["documents"]
    all_titles = [meta["title"].lower() for meta in collection.get()["metadatas"]]

    topic = query.lower()
    filtered_docs = [doc for doc, title in zip(all_docs, all_titles) if topic in title or topic in doc.lower()]
    
    if len(filtered_docs) == 0:
        filtered_docs = all_docs  # fallback: tÃ¼m dokÃ¼manlar

    filtered_docs = filtered_docs[:top_k]

    summarized_docs = [doc[:300] + "..." if len(doc) > 300 else doc for doc in filtered_docs]
    
    return summarized_docs

# ---------------------------
# 5ï¸âƒ£ Chat loop
# ---------------------------
print("ğŸ’¬ DOA Medical Chat'e hoÅŸ geldin! (Ã§Ä±kmak iÃ§in 'q' yaz)\n")

while True:
    user_input = input("ğŸ‘¤ Sen: ")
    if user_input.lower() in ["q", "quit", "exit"]:
        print("ğŸ”š Sohbet sonlandÄ±rÄ±ldÄ±.")
        break

    context_docs = retrieve_context(user_input)
    context_text = "\n\n".join(context_docs)

    prompt = f"""
AÅŸaÄŸÄ±daki tÄ±bbi makalelerden alÄ±nan bilgiler Ä±ÅŸÄ±ÄŸÄ±nda soruyu yanÄ±tla.
EÄŸer yeterli bilgi yoksa "Bu konuda yeterli veri bulunamadÄ±." de.

**Soru:** {user_input}

**BaÄŸlam:**
{context_text}
"""

    response = chat.send_message(prompt)
    print("ğŸ¤– Gemini:", response.text, "\n")
