# Chat.py

# Streamlit: web arayÃ¼zÃ¼ oluÅŸturmak iÃ§in kullanÄ±lÄ±r
import streamlit as st

# Google Generative AI: Gemini 2.0 Flash modelini kullanmak iÃ§in
import google.generativeai as genai

# HuggingFace datasets: parquet dosyalarÄ±nÄ± kolayca yÃ¼klemek iÃ§in
from datasets import load_dataset

# SentenceTransformers: metinleri vektÃ¶rlere Ã§evirip embedding oluÅŸturmak iÃ§in
from sentence_transformers import SentenceTransformer

# ChromaDB: embedding tabanlÄ± veritabanÄ±, hÄ±zlÄ± retrieval iÃ§in
import chromadb

# ---------------------------
# 1ï¸âƒ£ API Key
# ---------------------------

# Streamlit secrets.toml'dan API Key Ã§ekiyoruz
API = st.secrets["API_KEY"]

# Google Generative AI kÃ¼tÃ¼phanesini API Key ile yapÄ±landÄ±rÄ±yoruz
genai.configure(api_key=API)

# Gemini 2.0 Flash modelini baÅŸlatÄ±yoruz
model = genai.GenerativeModel("gemini-2.0-flash")

# Yeni bir chat oturumu baÅŸlatÄ±yoruz
chat = model.start_chat(history=[])

# ---------------------------
# 2ï¸âƒ£ Dataset yÃ¼kleme
# ---------------------------

# @st.cache_data: Fonksiyon sonucu cache'lenir. Tekrar tekrar yÃ¼klenmez, performans artar
@st.cache_data
def load_local_dataset():
    # Localdeki parquet dosyalarÄ±nÄ±n isimleri
    files = {
        "atlas": "atlas-00000-of-00001.parquet",
        "baskentistanbul": "baskentistanbul-00000-of-00001.parquet",
        "bayindir": "bayindir-00000-of-00001.parquet",
        "medipol": "medipol-00000-of-00001.parquet",
        "yeditepe": "yeditepe-00000-of-00001.parquet"
    }
    
    # HuggingFace datasets ile parquet dosyalarÄ±nÄ± yÃ¼kler
    dataset_raw = load_dataset("parquet", data_files=files)
    
    dataset = []  # Metinler buraya eklenecek
    titles = []   # BaÅŸlÄ±klar buraya eklenecek
    
    # Dataset iÃ§indeki her split'i dolaÅŸ
    for split in dataset_raw:
        # Sadece ilk 3 metni alÄ±yoruz (test amaÃ§lÄ±)
        dataset += dataset_raw[split][:3]["text"]
        # BaÅŸlÄ±klarÄ± oluÅŸturuyoruz
        titles += ["Makale " + str(i+1) for i in range(len(dataset_raw[split][:3]["text"]))]
    
    return dataset, titles  # Ä°ki listeyi dÃ¶ndÃ¼rÃ¼yoruz

# Fonksiyonu Ã§aÄŸÄ±r ve dataset ile titles deÄŸiÅŸkenlerini oluÅŸtur
dataset, titles = load_local_dataset()

# ---------------------------
# 3ï¸âƒ£ Embedding ve ChromaDB
# ---------------------------

# @st.cache_resource: Fonksiyon sonucu cache'lenir ve uzun sÃ¼reli kaynaklarÄ± (model, veritabanÄ±) saklar
@st.cache_resource
def init_chroma():
    # SentenceTransformer embedding modelini yÃ¼kle
    embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    
    # ChromaDB client oluÅŸtur
    client = chromadb.Client()
    
    # "medical_articles" koleksiyonunu oluÅŸtur veya varsa al
    collection = client.get_or_create_collection("medical_articles")
    
    # Koleksiyon boÅŸsa embedding oluÅŸtur ve ekle
    if len(collection.get()["ids"]) == 0:
        embeddings = embedding_model.encode(dataset, batch_size=32, show_progress_bar=True)
        for i, (text, title, emb) in enumerate(zip(dataset, titles, embeddings)):
            collection.add(
                ids=[str(i)],              # Her makaleye benzersiz ID
                embeddings=[emb],          # Embedding vektÃ¶rÃ¼
                documents=[text],          # AsÄ±l metin
                metadatas=[{"title": title}]  # BaÅŸlÄ±k metadata olarak
            )
    return collection, embedding_model

# Koleksiyon ve embedding modelini baÅŸlat
collection, embedding_model = init_chroma()

# ---------------------------
# 4ï¸âƒ£ Retrieval fonksiyonu
# ---------------------------

def retrieve_context(query, top_k=5):
    # TÃ¼m metinler ve baÅŸlÄ±klar
    all_docs = collection.get()["documents"]
    all_titles = [meta["title"].lower() for meta in collection.get()["metadatas"]]

    topic = query.lower()  # KullanÄ±cÄ± sorgusunu kÃ¼Ã§Ã¼k harfe Ã§evir
    
    # Sorguya uygun metinleri filtrele (baÅŸlÄ±k veya iÃ§erik eÅŸleÅŸirse)
    filtered_docs = [doc for doc, title in zip(all_docs, all_titles) if topic in title or topic in doc.lower()]
    
    # EÄŸer eÅŸleÅŸen yoksa tÃ¼m dokÃ¼manlarÄ± kullan
    if len(filtered_docs) == 0:
        filtered_docs = all_docs
    
    # Sadece top_k kadar dÃ¶ndÃ¼r
    filtered_docs = filtered_docs[:top_k]
    
    # Ã‡ok uzun dokÃ¼manlarÄ± kÄ±salt (ilk 300 karakter)
    summarized_docs = [doc[:300] + "..." if len(doc) > 300 else doc for doc in filtered_docs]
    
    return summarized_docs

# ---------------------------
# 5ï¸âƒ£ Streamlit arayÃ¼zÃ¼
# ---------------------------

# BaÅŸlÄ±k ekle
st.title("ğŸ’¬ DOA Medical Chat")

# KullanÄ±cÄ±dan metin al
user_input = st.text_input("Sorunuzu yazÄ±n:", "")

# EÄŸer kullanÄ±cÄ± yazdÄ±ysa
if user_input:
    # BaÄŸlamÄ± retrieve et
    context_docs = retrieve_context(user_input)
    context_text = "\n\n".join(context_docs)  # DokÃ¼manlarÄ± birleÅŸtir

    # Model iÃ§in prompt hazÄ±rla
    prompt = f"""
AÅŸaÄŸÄ±daki tÄ±bbi makalelerden alÄ±nan bilgiler Ä±ÅŸÄ±ÄŸÄ±nda soruyu yanÄ±tla.
EÄŸer yeterli bilgi yoksa "Bu konuda yeterli veri bulunamadÄ±." de.

**Soru:** {user_input}

**BaÄŸlam:**
{context_text}
"""
    # Gemini 2.0 Flash modeline prompt gÃ¶nder
    response = chat.send_message(prompt)
    
    # Model cevabÄ±nÄ± Streamlit Ã¼zerinde gÃ¶ster
    st.markdown(f"**ğŸ¤– Gemini:** {response.text}")
